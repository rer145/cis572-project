{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "path_to_csv = './data/'\n",
    "csvs = [pos_csv for pos_csv in os.listdir(path_to_csv) if pos_csv.endswith('.csv')]\n",
    "#csvs = ['knitting.csv']\n",
    "csv_dict = dict()\n",
    "for page in csvs:\n",
    "    #url = \"https://raw.githubusercontent.com/rer145/cis572-project/master/data/\"+str(page)+\".csv\"\n",
    "    #raw_data = requests.get(url).content\n",
    "    #df = pd.read_csv(io.StringIO(raw_data.decode('utf-8')))\n",
    "    df = pd.read_csv(path_to_csv + page)\n",
    "    csv_dict[page] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that aren't predictive\n",
    "# remove columns that are ngrams\n",
    "for key in csv_dict.keys():\n",
    "    csv_dict[key].drop('post_gilded', axis=1, inplace=True)\n",
    "    csv_dict[key].drop('post_gilded_silver', axis=1, inplace=True)\n",
    "    csv_dict[key].drop('post_gilded_gold', axis=1, inplace=True)\n",
    "    csv_dict[key].drop('post_gilded_platinum', axis=1, inplace=True)\n",
    "    csv_dict[key].drop('post_likes', axis=1, inplace=True)\n",
    "    csv_dict[key].drop('post_num_comments', axis=1, inplace=True)\n",
    "    csv_dict[key].drop('post_num_crossposts', axis=1, inplace=True)\n",
    "    csv_dict[key].drop('post_num_reports', axis=1, inplace=True)\n",
    "    csv_dict[key].drop('post_ups', axis=1, inplace=True)\n",
    "    csv_dict[key].drop('post_downs', axis=1, inplace=True)\n",
    "    \n",
    "    cols = [col for col in csv_dict[key].columns if '*' in col]\n",
    "    csv_dict[key].drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_score', 'post_archived', 'post_distinguished', 'post_edited',\n",
       "       'post_is_original_content', 'post_is_video', 'post_over_18',\n",
       "       'post_pinned', 'subreddit', 'subreddit_subscribers',\n",
       "       'posted_len_minutes', 'post_title_sentiment', 'post_title_characters',\n",
       "       'post_title_words', 'post_title_uniq_words', 'post_title_stop_words',\n",
       "       'post_title_non_stop_words', 'post_text_sentiment',\n",
       "       'post_text_characters', 'post_text_words', 'post_text_uniq_words',\n",
       "       'post_text_stop_words', 'post_text_non_stop_words', 'day_of_week'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dict['knitting.csv'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen.csv\n",
      "['post_archived', 'post_is_original_content', 'post_is_video', 'post_pinned', 'subreddit', 'subreddit_subscribers', 'day_of_week']\n",
      "\n",
      "askwomen.csv\n",
      "['post_archived', 'post_is_original_content', 'post_is_video', 'post_pinned', 'subreddit', 'day_of_week']\n",
      "\n",
      "aww-metadata.csv\n",
      "['post_archived', 'post_edited', 'post_over_18', 'post_pinned', 'subreddit', 'posted_len_minutes']\n",
      "\n",
      "aww-no-metadata.csv\n",
      "['post_archived', 'post_edited', 'post_over_18', 'post_pinned', 'subreddit', 'posted_len_minutes', 'post_text_sentiment', 'post_text_characters', 'post_text_words', 'post_text_uniq_words', 'post_text_stop_words', 'post_text_non_stop_words', 'day_of_week']\n",
      "\n",
      "aww.csv\n",
      "['post_archived', 'post_edited', 'post_over_18', 'post_pinned', 'subreddit', 'posted_len_minutes', 'post_text_sentiment', 'post_text_characters', 'post_text_words', 'post_text_uniq_words', 'post_text_stop_words', 'post_text_non_stop_words', 'day_of_week']\n",
      "\n",
      "conspiracy.csv\n",
      "['post_archived', 'post_is_original_content', 'post_is_video', 'post_pinned', 'subreddit', 'subreddit_subscribers', 'day_of_week']\n",
      "\n",
      "fitness.csv\n",
      "['post_archived', 'post_is_original_content', 'post_is_video', 'post_over_18', 'post_pinned', 'subreddit', 'day_of_week']\n",
      "\n",
      "knitting.csv\n",
      "['post_archived', 'post_is_original_content', 'post_pinned', 'subreddit', 'subreddit_subscribers', 'day_of_week']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove columns that only have a singular value\n",
    "for key in csv_dict.keys():\n",
    "    unique = [c for c in csv_dict[key].columns if len(set(csv_dict[key][c])) == 1]\n",
    "    print(key)\n",
    "    print(unique)\n",
    "    print()\n",
    "    csv_dict[key].drop(unique, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing askmen.csv\n",
      "parsing askwomen.csv\n",
      "parsing aww-metadata.csv\n",
      "parsing aww-no-metadata.csv\n",
      "parsing aww.csv\n",
      "parsing conspiracy.csv\n",
      "parsing fitness.csv\n",
      "parsing knitting.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for key in csv_dict.keys():\n",
    "    print('parsing', key)\n",
    "    #Change post_distinguished to numerical values, only None and Moderator values\n",
    "    if 'post_distinguished' in csv_dict[key]:\n",
    "        csv_dict[key]['post_distinguished'] = csv_dict[key]['post_distinguished'].map({None: 0, 'moderator': 1})\n",
    "        \n",
    "    #Mark edited posts with a 1\n",
    "    if 'post_edited' in csv_dict[key]:\n",
    "        csv_dict[key]['post_edited'] = (csv_dict[key]['post_edited'] == 'False')*1\n",
    "        \n",
    "    #Make post_over_18 binary\n",
    "    if 'post_over_18' in csv_dict[key]:\n",
    "        csv_dict[key]['post_over_18'] = csv_dict[key]['post_over_18'].astype('category').cat.codes\n",
    "        \n",
    "    #csv_dict[key] = csv_dict[key].dropna(csv_dict[key].median())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddit: askmen.csv \t Number of popular posts: 248 \t Popular score cutoff: 19\n",
      "Subreddit: askwomen.csv \t Number of popular posts: 248 \t Popular score cutoff: 21\n",
      "Subreddit: aww-metadata.csv \t Number of popular posts: 247 \t Popular score cutoff: 36\n",
      "Subreddit: aww-no-metadata.csv \t Number of popular posts: 247 \t Popular score cutoff: 36\n",
      "Subreddit: aww.csv \t Number of popular posts: 247 \t Popular score cutoff: 36\n",
      "Subreddit: conspiracy.csv \t Number of popular posts: 247 \t Popular score cutoff: 39\n",
      "Subreddit: fitness.csv \t Number of popular posts: 239 \t Popular score cutoff: 14\n",
      "Subreddit: knitting.csv \t Number of popular posts: 249 \t Popular score cutoff: 112\n"
     ]
    }
   ],
   "source": [
    "for key in csv_dict.keys():\n",
    "    csv_dict[key] = csv_dict[key].sort_values('post_score', ascending=False)\n",
    "    cutoff_pos = round(csv_dict[key].shape[0]*.25)\n",
    "    cutoff_score = csv_dict[key].iloc[cutoff_pos]['post_score']\n",
    "    print('Subreddit:',key,'\\t Number of popular posts:',cutoff_pos,'\\t Popular score cutoff:',int(cutoff_score))\n",
    "    \n",
    "    csv_dict[key].loc[csv_dict[key]['post_score'] >= cutoff_score, 'popular'] = 1\n",
    "    csv_dict[key].loc[csv_dict[key]['post_score'] < cutoff_score, 'popular'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    #remove cols\n",
    "    if 'post_score' in data.columns:\n",
    "        data.drop('post_score', axis=1, inplace=True)\n",
    "    \n",
    "    #split into test and training\n",
    "    X = data.drop('popular', axis=1)\n",
    "    y = data['popular']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)\n",
    "    \n",
    "    #split into validation and training\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=50)\n",
    "    \n",
    "    #scale the data\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #print('Data sizes')\n",
    "    #print('X_train', X_train.shape)\n",
    "    #print('X_val', X_val.shape)\n",
    "    #print('X_test', X_test.shape)\n",
    "    #print('y_train', y_train.shape)\n",
    "    #print('y_val', y_val.shape)\n",
    "    #print('y_test', y_test.shape)\n",
    "    \n",
    "    #return data\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "gnb = GaussianNB()\n",
    "knn = KNeighborsClassifier()\n",
    "svc = SVC()\n",
    "\n",
    "classifiers = [dtc, rfc, gnb, knn, svc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Instances</th>\n",
       "      <th># Features</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <th>K-Nearest Neighbors</th>\n",
       "      <th>Support Vector Classifer</th>\n",
       "      <th>Hard Voting</th>\n",
       "      <th>Soft Voting</th>\n",
       "      <th>Bagging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>askmen.csv</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>askwomen.csv</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aww-metadata.csv</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aww-no-metadata.csv</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aww.csv</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    # Instances # Features Decision Tree Random Forest  \\\n",
       "askmen.csv                  NaN        NaN           NaN           NaN   \n",
       "askwomen.csv                NaN        NaN           NaN           NaN   \n",
       "aww-metadata.csv            NaN        NaN           NaN           NaN   \n",
       "aww-no-metadata.csv         NaN        NaN           NaN           NaN   \n",
       "aww.csv                     NaN        NaN           NaN           NaN   \n",
       "\n",
       "                    Gaussian Naive Bayes K-Nearest Neighbors  \\\n",
       "askmen.csv                           NaN                 NaN   \n",
       "askwomen.csv                         NaN                 NaN   \n",
       "aww-metadata.csv                     NaN                 NaN   \n",
       "aww-no-metadata.csv                  NaN                 NaN   \n",
       "aww.csv                              NaN                 NaN   \n",
       "\n",
       "                    Support Vector Classifer Hard Voting Soft Voting Bagging  \n",
       "askmen.csv                               NaN         NaN         NaN     NaN  \n",
       "askwomen.csv                             NaN         NaN         NaN     NaN  \n",
       "aww-metadata.csv                         NaN         NaN         NaN     NaN  \n",
       "aww-no-metadata.csv                      NaN         NaN         NaN     NaN  \n",
       "aww.csv                                  NaN         NaN         NaN     NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['Decision Tree', 'Random Forest', 'Gaussian Naive Bayes', 'K-Nearest Neighbors', 'Support Vector Classifer']\n",
    "cols = ['# Instances', '# Features'] + names + ['Hard Voting', 'Soft Voting', 'Bagging']\n",
    "val_results = pd.DataFrame(index=csv_dict.keys(), columns=cols)\n",
    "val_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b3ade318e840>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Subreddit:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-30d71074c21f>\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'popular'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'popular'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#split into validation and training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "for key in csv_dict.keys():\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(csv_dict[key])\n",
    "    print('Subreddit:', key)\n",
    "    for name, algo in zip(names, classifiers):\n",
    "        algo.fit(X_train, y_train)\n",
    "        pred = algo.predict(X_val)\n",
    "        \n",
    "        val_results.loc[key]['# Instances'] = X_train.shape[0]\n",
    "        val_results.loc[key]['# Features'] = X_train.shape[1]\n",
    "        val_results.loc[key][name] = round(accuracy_score(y_val, pred), 3)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
