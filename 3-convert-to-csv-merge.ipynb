{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os \n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "\n",
    "def get_character_count(text):\n",
    "    return len(clean_text(text))\n",
    "\n",
    "def get_word_counts(text):\n",
    "    text = clean_text(text)\n",
    "    words = word_tokenize(text)\n",
    "    uniq_words = set(words)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    return len(words), len(uniq_words), len(uniq_words.intersection(stop_words)), len(uniq_words.difference(stop_words))\n",
    "\n",
    "def get_num_words(text):\n",
    "    text = clean_text(text)\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "def get_num_uniq_words(text):\n",
    "    text = clean_text(text)\n",
    "    words = word_tokenize(text)\n",
    "    uniq_words = set(words)\n",
    "    return len(uniq_words)\n",
    "\n",
    "def get_num_stop_words(text):\n",
    "    text = clean_text(text)\n",
    "    words = word_tokenize(text)\n",
    "    uniq_words = set(words)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    return len(uniq_words.intersection(stop_words))\n",
    "\n",
    "def get_num_non_stop_words(text):\n",
    "    text = clean_text(text)\n",
    "    words = word_tokenize(text)\n",
    "    uniq_words = set(words)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    return len(uniq_words.difference(stop_words))\n",
    "\n",
    "def get_day_of_week(date):\n",
    "    return date.weekday()\n",
    "\n",
    "def get_text_sentiment(text): \n",
    "    analysis = textblob.TextBlob(text) \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 0\n",
    "    else: \n",
    "        return -1\n",
    "    \n",
    "def get_ngrams(text, n):\n",
    "    text = clean_text(text)\n",
    "    words = word_tokenize(text)\n",
    "    tokens = set(words).difference(set(nltk.corpus.stopwords.words('english')))\n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_simple_data(data, is_original):\n",
    "    output = []\n",
    "\n",
    "    for item in data:\n",
    "        item['is_original'] = is_original\n",
    "        post_title = clean_text(item['post_title'].lower())\n",
    "        post_text = clean_text(item['post_selftext'].lower())\n",
    "        \n",
    "        if len(post_title) > 0:\n",
    "            item['post_title_sentiment'] = get_text_sentiment(post_title)\n",
    "            item['post_title_characters'] = get_character_count(post_title)\n",
    "            num_words, num_uniq_words, num_stop_words, num_non_stop_words = get_word_counts(post_title)\n",
    "            item['post_title_words'] = num_words\n",
    "            item['post_title_uniq_words'] = num_uniq_words\n",
    "            item['post_title_stop_words'] = num_stop_words\n",
    "            item['post_title_non_stop_words'] = num_non_stop_words\n",
    "        else:\n",
    "            item['post_title_sentiment'] = 0\n",
    "            item['post_title_characters'] = 0\n",
    "            item['post_title_words'] = 0\n",
    "            item['post_title_uniq_words'] = 0\n",
    "            item['post_title_stop_words'] = 0\n",
    "            item['post_title_non_stop_words'] = 0\n",
    "\n",
    "        if len(post_text) > 0:\n",
    "            item['post_text_sentiment'] = get_text_sentiment(post_text)\n",
    "            item['post_text_characters'] = get_character_count(post_text)\n",
    "            num_words, num_uniq_words, num_stop_words, num_non_stop_words = get_word_counts(post_text)\n",
    "            item['post_text_words'] = num_words\n",
    "            item['post_text_uniq_words'] = num_uniq_words\n",
    "            item['post_text_stop_words'] = num_stop_words\n",
    "            item['post_text_non_stop_words'] = num_non_stop_words\n",
    "        else:\n",
    "            item['post_text_sentiment'] = 0\n",
    "            item['post_text_characters'] = 0\n",
    "            item['post_text_words'] = 0\n",
    "            item['post_text_uniq_words'] = 0\n",
    "            item['post_text_stop_words'] = 0\n",
    "            item['post_text_non_stop_words'] = 0\n",
    "            \n",
    "        # calculate time frames\n",
    "        posted_on = item['post_created_utc']\n",
    "        captured_on = item['captured_on_utc']\n",
    "        d1 = datetime.strptime(datetime.utcfromtimestamp(posted_on).strftime('%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S')\n",
    "        d2 = datetime.strptime(captured_on[:-7], '%Y-%m-%d %H:%M:%S')\n",
    "        posted_len_minutes = abs((d1-d2).days * 24 * 60)\n",
    "        day_of_week = get_day_of_week(d2)\n",
    "\n",
    "        item['posted_len_minutes'] = posted_len_minutes\n",
    "        item['day_of_week'] = day_of_week\n",
    "\n",
    "        output.append(item)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_csv(data, include_header):\n",
    "    if include_header:\n",
    "        # Build CSV header\n",
    "        header = ['post_score', 'is_original', 'post_title', 'post_archived', 'post_distinguished', 'post_downs', 'post_edited', 'post_gilded', 'post_gilded_silver', \n",
    "            'post_gilded_gold', 'post_gilded_platinum', 'post_is_original_content', 'post_is_video', 'post_likes', \n",
    "            'post_num_comments', 'post_num_crossposts', 'post_num_reports', 'post_over_18', 'post_pinned', 'subreddit',\n",
    "            'subreddit_subscribers', 'post_ups', 'posted_len_minutes', 'post_title_sentiment', 'post_title_characters', \n",
    "            'post_title_words', 'post_title_uniq_words', 'post_title_stop_words', 'post_title_non_stop_words', \n",
    "            'post_text_sentiment', 'post_text_characters', 'post_text_words', 'post_text_uniq_words', \n",
    "            'post_text_stop_words', 'post_text_non_stop_words', 'day_of_week']\n",
    "\n",
    "        # Build CSV data\n",
    "        csv_data = [header]\n",
    "    else:\n",
    "        csv_data = []\n",
    "\n",
    "    for item in data:\n",
    "        row = []\n",
    "        row.append(item['post_score'])\n",
    "        row.append(item['is_original'])\n",
    "        row.append(item['post_title'])\n",
    "        row.append(item['post_archived'])\n",
    "        row.append(item['post_distinguished'])\n",
    "        row.append(item['post_downs'])\n",
    "        row.append(item['post_edited'])\n",
    "        row.append(item['post_gilded'])\n",
    "        row.append(item['post_gilded_silver'])\n",
    "        row.append(item['post_gilded_gold'])\n",
    "        row.append(item['post_gilded_platinum'])\n",
    "        row.append(item['post_is_original_content'])\n",
    "        row.append(item['post_is_video'])\n",
    "        row.append(item['post_likes'])\n",
    "        row.append(item['post_num_comments'])\n",
    "        row.append(item['post_num_crossposts'])\n",
    "        row.append(item['post_num_reports'])\n",
    "        row.append(item['post_over_18'])\n",
    "        row.append(item['post_pinned'])\n",
    "        row.append(item['subreddit'])\n",
    "        row.append(item['subreddit_subscribers'])\n",
    "        row.append(item['post_ups'])\n",
    "        row.append(item['posted_len_minutes'])\n",
    "        row.append(item['post_title_sentiment'])\n",
    "        row.append(item['post_title_characters'])\n",
    "        row.append(item['post_title_words'])\n",
    "        row.append(item['post_title_uniq_words'])\n",
    "        row.append(item['post_title_stop_words'])\n",
    "        row.append(item['post_title_non_stop_words'])\n",
    "        row.append(item['post_text_sentiment'])\n",
    "        row.append(item['post_text_characters'])\n",
    "        row.append(item['post_text_words'])\n",
    "        row.append(item['post_text_uniq_words'])\n",
    "        row.append(item['post_text_stop_words'])\n",
    "        row.append(item['post_text_non_stop_words'])\n",
    "        row.append(item['day_of_week'])\n",
    "        csv_data.append(row)\n",
    "    \n",
    "    \n",
    "    print(' Total records:', len(csv_data)-1)\n",
    "    print(' Total features:', len(csv_data[0]))\n",
    "    \n",
    "    return csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing subreddit askmen\n",
      " Total records: 991\n",
      " Total features: 36\n",
      "Parsing subreddit askwomen\n",
      " Total records: 991\n",
      " Total features: 36\n",
      "Parsing subreddit aww\n",
      " Total records: 987\n",
      " Total features: 36\n",
      "Parsing subreddit conspiracy\n",
      " Total records: 987\n",
      " Total features: 36\n",
      "Parsing subreddit fitness\n",
      " Total records: 955\n",
      " Total features: 36\n",
      "Parsing subreddit knitting\n",
      " Total records: 995\n",
      " Total features: 36\n",
      "All done parsing files\n"
     ]
    }
   ],
   "source": [
    "path_to_json = './data/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json') and not pos_json.endswith('-top.json')]\n",
    "#json_files = ['knitting.json']\n",
    "\n",
    "for file in json_files:\n",
    "    file = file[:-5]\n",
    "    \n",
    "    print('Parsing subreddit', file)\n",
    "    \n",
    "    with open('./data/' + file + '.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        new_data = format_simple_data(data, True)\n",
    "        \n",
    "    csv_data = build_csv(new_data, True)\n",
    "    with open('./data/' + file + '-simple.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(csv_data)\n",
    "        f.close()\n",
    "        \n",
    "print('All done parsing files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen-simple.csv\n",
      "askwomen-simple.csv\n",
      "aww-simple.csv\n",
      "conspiracy-simple.csv\n",
      "fitness-simple.csv\n",
      "knitting-simple.csv\n"
     ]
    }
   ],
   "source": [
    "path_to_csv = './data/'\n",
    "csvs = [pos_csv for pos_csv in os.listdir(path_to_csv) if pos_csv.endswith('-simple.csv')]\n",
    "csv_dict = dict()\n",
    "for page in csvs:\n",
    "    df = pd.read_csv(path_to_csv + page, low_memory=False)\n",
    "    csv_dict[page] = df\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen-simple.csv\n",
      " Total records: 480\n",
      " Total features: 36\n",
      "askwomen-simple.csv\n",
      " Total records: 480\n",
      " Total features: 36\n",
      "aww-simple.csv\n",
      " Total records: 480\n",
      " Total features: 36\n",
      "conspiracy-simple.csv\n",
      " Total records: 480\n",
      " Total features: 36\n",
      "fitness-simple.csv\n",
      " Total records: 480\n",
      " Total features: 36\n",
      "knitting-simple.csv\n",
      " Total records: 480\n",
      " Total features: 36\n"
     ]
    }
   ],
   "source": [
    "for key in csv_dict.keys():\n",
    "    df = csv_dict[key]\n",
    "    cutoff_score = csv_dict[key]['post_score'].describe()[6]\n",
    "    popular = len(csv_dict[key].loc[csv_dict[key]['post_score'] >= cutoff_score])\n",
    "    unpopular = len(csv_dict[key].loc[csv_dict[key]['post_score'] < cutoff_score])\n",
    "    \n",
    "    print(key)\n",
    "    #print('Top 25% cutoff', cutoff_score)\n",
    "    #print('Total Popular', popular)\n",
    "    #print('Total Not Popular', unpopular)\n",
    "    #print('New Popular posts needed', unpopular-popular)\n",
    "    #print()\n",
    "    \n",
    "    # fill data with top posts to make it 50-50\n",
    "    file = key[:-11]\n",
    "    with open('./data/' + file + '-top.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        new_data = format_simple_data(data, False)\n",
    "        \n",
    "    # get up to 480 posts that exceed the cutoff\n",
    "    top_data = []\n",
    "    counter = 0\n",
    "    for item in new_data:\n",
    "        if item['post_score'] >= cutoff_score:\n",
    "            top_data.append(item)\n",
    "            counter = counter+1\n",
    "            \n",
    "        if counter > 480:\n",
    "            break\n",
    "    \n",
    "    csv_data = build_csv(top_data, False)\n",
    "    with open('./data/' + file + '-simple.csv', 'a', newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(csv_data)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen-simple.csv\n",
      "cutoff 19.0\n",
      "Total Popular 734\n",
      "Total Not Popular 738\n",
      "\n",
      "askwomen-simple.csv\n",
      "cutoff 21.0\n",
      "Total Popular 732\n",
      "Total Not Popular 740\n",
      "\n",
      "aww-simple.csv\n",
      "cutoff 36.0\n",
      "Total Popular 731\n",
      "Total Not Popular 737\n",
      "\n",
      "conspiracy-simple.csv\n",
      "cutoff 39.0\n",
      "Total Popular 730\n",
      "Total Not Popular 738\n",
      "\n",
      "fitness-simple.csv\n",
      "cutoff 14.0\n",
      "Total Popular 723\n",
      "Total Not Popular 713\n",
      "\n",
      "knitting-simple.csv\n",
      "cutoff 112.0\n",
      "Total Popular 732\n",
      "Total Not Popular 744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_csv = './data/'\n",
    "csvs = [pos_csv for pos_csv in os.listdir(path_to_csv) if pos_csv.endswith('-simple.csv')]\n",
    "csv_dict = dict()\n",
    "for page in csvs:\n",
    "    df = pd.read_csv(path_to_csv + page, low_memory=False)\n",
    "    csv_dict[page] = df\n",
    "    \n",
    "    print(page)\n",
    "    df = csv_dict[page]\n",
    "    cutoff_score = df.loc[df['is_original'] == True]['post_score'].describe()[6]\n",
    "    \n",
    "    popular = len(df.loc[df['post_score'] >= cutoff_score])\n",
    "    unpopular = len(df.loc[df['post_score'] < cutoff_score])\n",
    "    \n",
    "    print('cutoff', cutoff_score)\n",
    "    print('Total Popular', popular)\n",
    "    print('Total Not Popular', unpopular)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_ngram_data(data):\n",
    "    w_bigrams = []\n",
    "    w_trigrams = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        wbg = get_ngrams(row['post_title'], 2)\n",
    "        wtg = get_ngrams(row['post_title'], 3)\n",
    "    \n",
    "        w_bigrams = w_bigrams + wbg\n",
    "        w_trigrams = w_trigrams + wtg\n",
    "        \n",
    "    w_bigrams_uniq = set(w_bigrams)\n",
    "    w_trigrams_uniq = set(w_trigrams)\n",
    "    \n",
    "    # add all columns and set to 0\n",
    "    for item in w_bigrams_uniq:\n",
    "        data[item] = 0\n",
    "        \n",
    "    for item in w_trigrams_uniq:\n",
    "        data[item] = 0\n",
    "        \n",
    "    # loop through all records again to set to 1 if applicable\n",
    "    for index, row in data.iterrows():\n",
    "        wbg = get_ngrams(row['post_title'], 2)\n",
    "        wtg = get_ngrams(row['post_title'], 3)\n",
    "        \n",
    "        for item in w_bigrams_uniq:\n",
    "            if item in wbg:\n",
    "                data.loc[index, item] = 1\n",
    "                \n",
    "        for item in w_trigrams_uniq:\n",
    "            if item in wtg:\n",
    "                data.loc[index, item] = 1\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askmen-simple.csv\n",
      "Generating ngrams\n",
      "Saving file\n",
      "done saving ./data/askmen-full.csv\n",
      "\n",
      "askwomen-simple.csv\n",
      "Generating ngrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:58: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file\n",
      "done saving ./data/askwomen-full.csv\n",
      "\n",
      "aww-simple.csv\n",
      "Generating ngrams\n",
      "Saving file\n",
      "done saving ./data/aww-full.csv\n",
      "\n",
      "conspiracy-simple.csv\n",
      "Generating ngrams\n",
      "Saving file\n",
      "done saving ./data/conspiracy-full.csv\n",
      "\n",
      "fitness-simple.csv\n",
      "Generating ngrams\n",
      "Saving file\n",
      "done saving ./data/fitness-full.csv\n",
      "\n",
      "knitting-simple.csv\n",
      "Generating ngrams\n",
      "Saving file\n",
      "done saving ./data/knitting-full.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate ngrams and add to csvs\n",
    "path_to_csv = './data/'\n",
    "csvs = [pos_csv for pos_csv in os.listdir(path_to_csv) if pos_csv.endswith('-simple.csv')]\n",
    "\n",
    "csv_dict = dict()\n",
    "for key in csvs:\n",
    "    df = pd.read_csv(path_to_csv + key, low_memory=False)\n",
    "    print(key)\n",
    "    print('Generating ngrams')\n",
    "    csv_dict[key] = append_ngram_data(df)\n",
    "    \n",
    "    print('Saving file')\n",
    "    save_path = './data/' + key[:-11] + '-full.csv'\n",
    "    csv_dict[key].to_csv(path_or_buf=save_path, index=False)\n",
    "    print('done saving', save_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
