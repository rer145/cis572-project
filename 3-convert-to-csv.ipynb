{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os \n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "\n",
    "def get_character_count(text):\n",
    "    return len(clean_text(text))\n",
    "\n",
    "def get_word_counts(text):\n",
    "    text = clean_text(text)\n",
    "    words = word_tokenize(text)\n",
    "    uniq_words = set(words)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    return len(words), len(uniq_words), len(uniq_words.intersection(stop_words)), len(uniq_words.difference(stop_words))\n",
    "\n",
    "def get_day_of_week(date):\n",
    "    return date.weekday()\n",
    "\n",
    "def get_text_sentiment(text): \n",
    "    analysis = textblob.TextBlob(text) \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 0\n",
    "    else: \n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing subreddit askmen\n",
      "Parsing subreddit askwomen\n",
      "Parsing subreddit aww-no-metadata\n",
      "Parsing subreddit aww\n",
      "Parsing subreddit conspiracy\n",
      "Parsing subreddit fitness\n",
      "Parsing subreddit knitting\n",
      "All done parsing files\n"
     ]
    }
   ],
   "source": [
    "path_to_json = './data/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "#json_files = ['knitting.json']\n",
    "\n",
    "for file in json_files:\n",
    "    file = file[:-5]\n",
    "    \n",
    "    print('Parsing subreddit', file)\n",
    "    \n",
    "    with open('./data/' + file + '.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    new_data = []\n",
    "\n",
    "    for item in data:\n",
    "        post_title = clean_text(item['post_title'])\n",
    "        post_text = clean_text(item['post_selftext'])\n",
    "        \n",
    "        if len(post_title) > 0:\n",
    "            item['post_title_sentiment'] = get_text_sentiment(post_title)\n",
    "            item['post_title_characters'] = get_character_count(post_title)\n",
    "            num_words, num_uniq_words, num_stop_words, num_non_stop_words = get_word_counts(post_title)\n",
    "            item['post_title_words'] = num_words\n",
    "            item['post_title_uniq_words'] = num_uniq_words\n",
    "            item['post_title_stop_words'] = num_stop_words\n",
    "            item['post_title_non_stop_words'] = num_non_stop_words\n",
    "        else:\n",
    "            item['post_title_sentiment'] = 0\n",
    "            item['post_title_characters'] = 0\n",
    "            item['post_title_words'] = 0\n",
    "            item['post_title_uniq_words'] = 0\n",
    "            item['post_title_stop_words'] = 0\n",
    "            item['post_title_non_stop_words'] = 0\n",
    "\n",
    "        if len(post_text) > 0:\n",
    "            item['post_text_sentiment'] = get_text_sentiment(post_text)\n",
    "            item['post_text_characters'] = get_character_count(post_text)\n",
    "            num_words, num_uniq_words, num_stop_words, num_non_stop_words = get_word_counts(post_text)\n",
    "            item['post_text_words'] = num_words\n",
    "            item['post_text_uniq_words'] = num_uniq_words\n",
    "            item['post_text_stop_words'] = num_stop_words\n",
    "            item['post_text_non_stop_words'] = num_non_stop_words\n",
    "        else:\n",
    "            item['post_text_sentiment'] = 0\n",
    "            item['post_text_characters'] = 0\n",
    "            item['post_text_words'] = 0\n",
    "            item['post_text_uniq_words'] = 0\n",
    "            item['post_text_stop_words'] = 0\n",
    "            item['post_text_non_stop_words'] = 0\n",
    "            \n",
    "        \n",
    "        #if 'metadata' in item.keys():\n",
    "        #    metadata = metadata + item['metadata']\n",
    "        #else:\n",
    "        #    item['metadata'] = []\n",
    "        \n",
    "\n",
    "        # calculate time frames\n",
    "        posted_on = item['post_created_utc']\n",
    "        captured_on = item['captured_on_utc']\n",
    "        d1 = datetime.strptime(datetime.utcfromtimestamp(posted_on).strftime('%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S')\n",
    "        d2 = datetime.strptime(captured_on[:-7], '%Y-%m-%d %H:%M:%S')\n",
    "        posted_len_minutes = abs((d1-d2).days * 24 * 60)\n",
    "        day_of_week = get_day_of_week(d2)\n",
    "\n",
    "        item['posted_len_minutes'] = posted_len_minutes\n",
    "        item['day_of_week'] = day_of_week\n",
    "\n",
    "        new_data.append(item)\n",
    "\n",
    "        \n",
    "\n",
    "    # Build CSV header\n",
    "    header = ['post_score', 'post_archived', 'post_distinguished', 'post_downs', 'post_edited', 'post_gilded', 'post_gilded_silver', \n",
    "        'post_gilded_gold', 'post_gilded_platinum', 'post_is_original_content', 'post_is_video', 'post_likes', \n",
    "        'post_num_comments', 'post_num_crossposts', 'post_num_reports', 'post_over_18', 'post_pinned', 'subreddit',\n",
    "        'subreddit_subscribers', 'post_ups', 'posted_len_minutes', 'post_title_sentiment', 'post_title_characters', \n",
    "        'post_title_words', 'post_title_uniq_words', 'post_title_stop_words', 'post_title_non_stop_words', \n",
    "        'post_text_sentiment', 'post_text_characters', 'post_text_words', 'post_text_uniq_words', \n",
    "        'post_text_stop_words', 'post_text_non_stop_words', 'day_of_week']\n",
    "\n",
    "    \n",
    "    \n",
    "    # Build CSV data\n",
    "    csv_data = [header]\n",
    "\n",
    "    for item in new_data:\n",
    "        row = []\n",
    "        row.append(item['post_score'])\n",
    "        row.append(item['post_archived'])\n",
    "        row.append(item['post_distinguished'])\n",
    "        row.append(item['post_downs'])\n",
    "        row.append(item['post_edited'])\n",
    "        row.append(item['post_gilded'])\n",
    "        row.append(item['post_gilded_silver'])\n",
    "        row.append(item['post_gilded_gold'])\n",
    "        row.append(item['post_gilded_platinum'])\n",
    "        row.append(item['post_is_original_content'])\n",
    "        row.append(item['post_is_video'])\n",
    "        row.append(item['post_likes'])\n",
    "        row.append(item['post_num_comments'])\n",
    "        row.append(item['post_num_crossposts'])\n",
    "        row.append(item['post_num_reports'])\n",
    "        row.append(item['post_over_18'])\n",
    "        row.append(item['post_pinned'])\n",
    "        row.append(item['subreddit'])\n",
    "        row.append(item['subreddit_subscribers'])\n",
    "        row.append(item['post_ups'])\n",
    "        row.append(item['posted_len_minutes'])\n",
    "        row.append(item['post_title_sentiment'])\n",
    "        row.append(item['post_title_characters'])\n",
    "        row.append(item['post_title_words'])\n",
    "        row.append(item['post_title_uniq_words'])\n",
    "        row.append(item['post_title_stop_words'])\n",
    "        row.append(item['post_title_non_stop_words'])\n",
    "        row.append(item['post_text_sentiment'])\n",
    "        row.append(item['post_text_characters'])\n",
    "        row.append(item['post_text_words'])\n",
    "        row.append(item['post_text_uniq_words'])\n",
    "        row.append(item['post_text_stop_words'])\n",
    "        row.append(item['post_text_non_stop_words'])\n",
    "        row.append(item['day_of_week'])\n",
    "        \n",
    "        csv_data.append(row)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Write CSV data to a file\n",
    "    with open('./data/' + file + '.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(csv_data)\n",
    "        f.close()\n",
    "        \n",
    "print('All done parsing files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
