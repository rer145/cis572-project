{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os \n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "\n",
    "def get_character_count(text):\n",
    "    return len(clean_text(text))\n",
    "\n",
    "def get_word_counts(text):\n",
    "    text = clean_text(text)\n",
    "    words = word_tokenize(text)\n",
    "    uniq_words = set(words)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    return len(words), len(uniq_words), len(uniq_words.intersection(stop_words)), len(uniq_words.difference(stop_words))\n",
    "\n",
    "def get_day_of_week(date):\n",
    "    return date.weekday()\n",
    "\n",
    "def get_text_sentiment(text): \n",
    "    analysis = textblob.TextBlob(text) \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 0\n",
    "    else: \n",
    "        return -1\n",
    "    \n",
    "def get_ngrams(text, n):\n",
    "    text = clean_text(text)\n",
    "    words = word_tokenize(text)\n",
    "    tokens = set(words).difference(set(nltk.corpus.stopwords.words('english')))\n",
    "    return list(ngrams(tokens, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing subreddit askmen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:31: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " post title ngram totals:\n",
      "   askmen\n",
      "   Bigrams 5082\n",
      "   Trigrams 4096\n",
      "   Quadgrams 3154\n",
      "\n",
      " post title unique ngram totals:\n",
      "   askmen\n",
      "   Bigrams 4858\n",
      "   Trigrams 4091\n",
      "   Quadgrams 3152\n",
      " Total records: 991\n",
      " Total features: 12135\n",
      "Parsing subreddit askwomen\n",
      " post title ngram totals:\n",
      "   askwomen\n",
      "   Bigrams 4662\n",
      "   Trigrams 3683\n",
      "   Quadgrams 2765\n",
      "\n",
      " post title unique ngram totals:\n",
      "   askwomen\n",
      "   Bigrams 4438\n",
      "   Trigrams 3663\n",
      "   Quadgrams 2750\n",
      " Total records: 991\n",
      " Total features: 10885\n",
      "Parsing subreddit aww-no-metadata\n",
      " post title ngram totals:\n",
      "   aww-no-metadata\n",
      "   Bigrams 3389\n",
      "   Trigrams 2498\n",
      "   Quadgrams 1824\n",
      "\n",
      " post title unique ngram totals:\n",
      "   aww-no-metadata\n",
      "   Bigrams 3266\n",
      "   Trigrams 2491\n",
      "   Quadgrams 1822\n",
      " Total records: 987\n",
      " Total features: 7613\n",
      "Parsing subreddit aww\n",
      " post title ngram totals:\n",
      "   aww\n",
      "   Bigrams 3389\n",
      "   Trigrams 2498\n",
      "   Quadgrams 1824\n",
      "\n",
      " post title unique ngram totals:\n",
      "   aww\n",
      "   Bigrams 3266\n",
      "   Trigrams 2491\n",
      "   Quadgrams 1822\n",
      " Total records: 987\n",
      " Total features: 7613\n",
      "Parsing subreddit conspiracy\n",
      " post title ngram totals:\n",
      "   conspiracy\n",
      "   Bigrams 8025\n",
      "   Trigrams 7050\n",
      "   Quadgrams 6119\n",
      "\n",
      " post title unique ngram totals:\n",
      "   conspiracy\n",
      "   Bigrams 7774\n",
      "   Trigrams 6932\n",
      "   Quadgrams 6016\n"
     ]
    }
   ],
   "source": [
    "path_to_json = './data/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "#json_files = ['knitting.json']\n",
    "\n",
    "for file in json_files:\n",
    "    file = file[:-5]\n",
    "    \n",
    "    print('Parsing subreddit', file)\n",
    "    \n",
    "    with open('./data/' + file + '.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    new_data = []\n",
    "    \n",
    "    w_bigrams = []\n",
    "    w_trigrams = []\n",
    "    w_quadgrams = []\n",
    "\n",
    "    for item in data:\n",
    "        post_title = clean_text(item['post_title'].lower())\n",
    "        post_text = clean_text(item['post_selftext'].lower())\n",
    "        \n",
    "        if len(post_title) > 0:\n",
    "            item['post_title_sentiment'] = get_text_sentiment(post_title)\n",
    "            item['post_title_characters'] = get_character_count(post_title)\n",
    "            num_words, num_uniq_words, num_stop_words, num_non_stop_words = get_word_counts(post_title)\n",
    "            item['post_title_words'] = num_words\n",
    "            item['post_title_uniq_words'] = num_uniq_words\n",
    "            item['post_title_stop_words'] = num_stop_words\n",
    "            item['post_title_non_stop_words'] = num_non_stop_words\n",
    "            \n",
    "            # get ngrams\n",
    "            wbg = get_ngrams(post_title, 2)\n",
    "            wtg = get_ngrams(post_title, 3)\n",
    "            wqg = get_ngrams(post_title, 4)\n",
    "            \n",
    "            w_bigrams = w_bigrams + wbg\n",
    "            w_trigrams = w_trigrams + wtg\n",
    "            w_quadgrams = w_quadgrams + wqg\n",
    "            \n",
    "            item['post_title_bigrams'] = wbg\n",
    "            item['post_title_trigrams'] = wtg\n",
    "            item['post_title_quadgrams'] = wqg\n",
    "\n",
    "        else:\n",
    "            item['post_title_sentiment'] = 0\n",
    "            item['post_title_characters'] = 0\n",
    "            item['post_title_words'] = 0\n",
    "            item['post_title_uniq_words'] = 0\n",
    "            item['post_title_stop_words'] = 0\n",
    "            item['post_title_non_stop_words'] = 0\n",
    "            item['post_title_bigrams'] = []\n",
    "            item['post_title_trigrams'] = []\n",
    "            item['post_title_quadgrams'] = []\n",
    "\n",
    "        if len(post_text) > 0:\n",
    "            item['post_text_sentiment'] = get_text_sentiment(post_text)\n",
    "            item['post_text_characters'] = get_character_count(post_text)\n",
    "            num_words, num_uniq_words, num_stop_words, num_non_stop_words = get_word_counts(post_text)\n",
    "            item['post_text_words'] = num_words\n",
    "            item['post_text_uniq_words'] = num_uniq_words\n",
    "            item['post_text_stop_words'] = num_stop_words\n",
    "            item['post_text_non_stop_words'] = num_non_stop_words\n",
    "        else:\n",
    "            item['post_text_sentiment'] = 0\n",
    "            item['post_text_characters'] = 0\n",
    "            item['post_text_words'] = 0\n",
    "            item['post_text_uniq_words'] = 0\n",
    "            item['post_text_stop_words'] = 0\n",
    "            item['post_text_non_stop_words'] = 0\n",
    "            \n",
    "        \n",
    "        #if 'metadata' in item.keys():\n",
    "        #    metadata = metadata + item['metadata']\n",
    "        #else:\n",
    "        #    item['metadata'] = []\n",
    "        \n",
    "\n",
    "        # calculate time frames\n",
    "        posted_on = item['post_created_utc']\n",
    "        captured_on = item['captured_on_utc']\n",
    "        d1 = datetime.strptime(datetime.utcfromtimestamp(posted_on).strftime('%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S')\n",
    "        d2 = datetime.strptime(captured_on[:-7], '%Y-%m-%d %H:%M:%S')\n",
    "        posted_len_minutes = abs((d1-d2).days * 24 * 60)\n",
    "        day_of_week = get_day_of_week(d2)\n",
    "\n",
    "        item['posted_len_minutes'] = posted_len_minutes\n",
    "        item['day_of_week'] = day_of_week\n",
    "\n",
    "        new_data.append(item)\n",
    "    \n",
    "    \n",
    "    # summary data for each file\n",
    "    w_bigrams_uniq = set(w_bigrams)\n",
    "    w_trigrams_uniq = set(w_trigrams)\n",
    "    w_quadgrams_uniq = set(w_quadgrams)\n",
    "\n",
    "    print(' post title ngram totals:')\n",
    "    print('  ', file)\n",
    "    print('   Bigrams', len(w_bigrams))\n",
    "    print('   Trigrams', len(w_trigrams))\n",
    "    print('   Quadgrams', len(w_quadgrams))\n",
    "    print()\n",
    "    print(' post title unique ngram totals:')\n",
    "    print('  ', file)\n",
    "    print('   Bigrams', len(w_bigrams_uniq))\n",
    "    print('   Trigrams', len(w_trigrams_uniq))\n",
    "    print('   Quadgrams', len(w_quadgrams_uniq))\n",
    "\n",
    "        \n",
    "\n",
    "    # Build CSV header\n",
    "    header = ['post_score', 'post_archived', 'post_distinguished', 'post_downs', 'post_edited', 'post_gilded', 'post_gilded_silver', \n",
    "        'post_gilded_gold', 'post_gilded_platinum', 'post_is_original_content', 'post_is_video', 'post_likes', \n",
    "        'post_num_comments', 'post_num_crossposts', 'post_num_reports', 'post_over_18', 'post_pinned', 'subreddit',\n",
    "        'subreddit_subscribers', 'post_ups', 'posted_len_minutes', 'post_title_sentiment', 'post_title_characters', \n",
    "        'post_title_words', 'post_title_uniq_words', 'post_title_stop_words', 'post_title_non_stop_words', \n",
    "        'post_text_sentiment', 'post_text_characters', 'post_text_words', 'post_text_uniq_words', \n",
    "        'post_text_stop_words', 'post_text_non_stop_words', 'day_of_week']\n",
    "    \n",
    "    static_header_count = len(header)\n",
    "    \n",
    "    header = header + [x[0] + '*' + x[1] for x in list(w_bigrams_uniq)]\n",
    "    header = header + [x[0] + '*' + x[1] + '*' + x[2] for x in list(w_trigrams_uniq)]\n",
    "    header = header + [x[0] + '*' + x[1] + '*' + x[2] + '*' + x[3] for x in list(w_quadgrams_uniq)]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Build CSV data\n",
    "    csv_data = [header]\n",
    "\n",
    "    for item in new_data:\n",
    "        row = []\n",
    "        row.append(item['post_score'])\n",
    "        row.append(item['post_archived'])\n",
    "        row.append(item['post_distinguished'])\n",
    "        row.append(item['post_downs'])\n",
    "        row.append(item['post_edited'])\n",
    "        row.append(item['post_gilded'])\n",
    "        row.append(item['post_gilded_silver'])\n",
    "        row.append(item['post_gilded_gold'])\n",
    "        row.append(item['post_gilded_platinum'])\n",
    "        row.append(item['post_is_original_content'])\n",
    "        row.append(item['post_is_video'])\n",
    "        row.append(item['post_likes'])\n",
    "        row.append(item['post_num_comments'])\n",
    "        row.append(item['post_num_crossposts'])\n",
    "        row.append(item['post_num_reports'])\n",
    "        row.append(item['post_over_18'])\n",
    "        row.append(item['post_pinned'])\n",
    "        row.append(item['subreddit'])\n",
    "        row.append(item['subreddit_subscribers'])\n",
    "        row.append(item['post_ups'])\n",
    "        row.append(item['posted_len_minutes'])\n",
    "        row.append(item['post_title_sentiment'])\n",
    "        row.append(item['post_title_characters'])\n",
    "        row.append(item['post_title_words'])\n",
    "        row.append(item['post_title_uniq_words'])\n",
    "        row.append(item['post_title_stop_words'])\n",
    "        row.append(item['post_title_non_stop_words'])\n",
    "        row.append(item['post_text_sentiment'])\n",
    "        row.append(item['post_text_characters'])\n",
    "        row.append(item['post_text_words'])\n",
    "        row.append(item['post_text_uniq_words'])\n",
    "        row.append(item['post_text_stop_words'])\n",
    "        row.append(item['post_text_non_stop_words'])\n",
    "        row.append(item['day_of_week'])\n",
    "        \n",
    "        for x in header[static_header_count:]:\n",
    "            temp = x.split('*')\n",
    "            contains_ngram = False\n",
    "\n",
    "            if len(temp) == 2:\n",
    "                for y in item['post_title_bigrams']:\n",
    "                    if y[0] == temp[0] and y[1] == temp[1]:\n",
    "                        contains_ngram = True\n",
    "            if len(temp) == 3:\n",
    "                for y in item['post_title_trigrams']:\n",
    "                    if y[0] == temp[0] and y[1] == temp[1] and y[2] == temp[2]:\n",
    "                        contains_ngram = True\n",
    "            if len(temp) == 4:\n",
    "                for y in item['post_title_quadgrams']:\n",
    "                    if y[0] == temp[0] and y[1] == temp[1] and y[2] == temp[2] and y[3] == temp[3]:\n",
    "                        contains_ngram = True\n",
    "                        \n",
    "            row.append(int(contains_ngram))\n",
    "        \n",
    "        csv_data.append(row)\n",
    "    \n",
    "    \n",
    "    print(' Total records:', len(csv_data)-1)\n",
    "    print(' Total features:', len(csv_data[0]))\n",
    "    \n",
    "    # Write CSV data to a file\n",
    "    with open('./data/' + file + '.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(csv_data)\n",
    "        f.close()\n",
    "        \n",
    "print('All done parsing files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
