{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os \n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata 0\n",
      "Bigrams 9773\n",
      "Trigrams 10637\n",
      "Quadgrams 10438\n"
     ]
    }
   ],
   "source": [
    "path_to_json = './data/'\n",
    "#json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "json_files = ['knitting.json']\n",
    "\n",
    "for file in json_files:\n",
    "    file = file[:-5]\n",
    "    with open('./data/' + file + '.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    \n",
    "    new_data = []\n",
    "\n",
    "    w_bigrams = []\n",
    "    w_trigrams = []\n",
    "    w_quadgrams = []\n",
    "    metadata = []\n",
    "\n",
    "    for item in data:\n",
    "        post_text = item['post_selftext']\n",
    "        wbg = []\n",
    "        wtg = []\n",
    "        wqg = []\n",
    "\n",
    "        # tokenize and ngramify post text if available\n",
    "        if len(post_text) > 0:\n",
    "            post_text = re.sub('https?:\\/{2}[\\d\\w]+\\.([\\d\\w]+)*(\\/[^\\s]*)*', '', post_text)\n",
    "            post_text = re.sub(r'[^\\w\\s]', '', post_text)\n",
    "\n",
    "            #emoji_pattern = re.compile(\"[\"\n",
    "            #    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            #    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            #    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            #    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            #                       \"]+\", flags=re.UNICODE)\n",
    "            #post_text = emoji_pattern.sub(r'', post_text)\n",
    "\n",
    "            words = word_tokenize(post_text)\n",
    "            words_minus_stop_words = set(words).difference(set(nltk.corpus.stopwords.words('english')))\n",
    "            wnl = nltk.WordNetLemmatizer()\n",
    "            tokens = [wnl.lemmatize(t) for t in words_minus_stop_words]\n",
    "\n",
    "            wbg = list(ngrams(tokens, 2))\n",
    "            wtg = list(ngrams(tokens, 3))\n",
    "            wqg = list(ngrams(tokens, 4))\n",
    "\n",
    "            w_bigrams = w_bigrams + wbg\n",
    "            w_trigrams = w_trigrams + wtg\n",
    "            w_quadgrams = w_quadgrams + wqg\n",
    "\n",
    "        # collect all metadata tags\n",
    "        if 'metadata' in item.keys():\n",
    "            metadata = metadata + item['metadata']\n",
    "        else:\n",
    "            item['metadata'] = []\n",
    "\n",
    "        # calculate time frames\n",
    "        posted_on = item['post_created_utc']\n",
    "        captured_on = item['captured_on_utc']\n",
    "        d1 = datetime.strptime(datetime.utcfromtimestamp(posted_on).strftime('%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S')\n",
    "        d2 = datetime.strptime(captured_on[:-7], '%Y-%m-%d %H:%M:%S')\n",
    "        posted_len_minutes = abs((d1-d2).days * 24 * 60)\n",
    "\n",
    "        item['posted_len_minutes'] = posted_len_minutes\n",
    "        item['word_bigrams'] = wbg\n",
    "        item['word_trigrams'] = wtg\n",
    "        item['word_quadgrams'] = wqg\n",
    "\n",
    "        new_data.append(item)\n",
    "\n",
    "    uniq_metadata = set(metadata)\n",
    "    uniq_bigrams = set(w_bigrams)\n",
    "    uniq_trigrams = set(w_trigrams)\n",
    "    uniq_quadgrams = set(w_quadgrams)\n",
    "    \n",
    "    print('Metadata', len(uniq_metadata))\n",
    "    print('Bigrams', len(uniq_bigrams))\n",
    "    print('Trigrams', len(uniq_trigrams))\n",
    "    print('Quadgrams', len(uniq_quadgrams))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Build CSV header\n",
    "    header = ['post_score', 'post_archived', 'post_distinguished', 'post_downs', 'post_edited', 'post_gilded', 'post_gilded_silver', \n",
    "        'post_gilded_gold', 'post_gilded_platinum', 'post_is_original_content', 'post_is_video', 'post_likes', \n",
    "        'post_num_comments', 'post_num_crossposts', 'post_num_reports', 'post_over_18', 'post_pinned', 'subreddit',\n",
    "        'subreddit_subscribers', 'post_ups', 'posted_len_minutes']\n",
    "\n",
    "    header = header + [x[0] + '*' + x[1] for x in list(uniq_bigrams)]\n",
    "    header = header + [x[0] + '*' + x[1] + '*' + x[2] for x in list(uniq_trigrams)]\n",
    "    header = header + [x[0] + '*' + x[1] + '*' + x[2] + '*' + x[3] for x in list(uniq_quadgrams)]\n",
    "    header = header + [x for x in list(uniq_metadata)]\n",
    "\n",
    "\n",
    "    # Build CSV data\n",
    "    csv_data = [header]\n",
    "\n",
    "    for item in new_data:\n",
    "        row = []\n",
    "        row.append(item['post_score'])\n",
    "        row.append(item['post_archived'])\n",
    "        row.append(item['post_distinguished'])\n",
    "        row.append(item['post_downs'])\n",
    "        row.append(item['post_edited'])\n",
    "        row.append(item['post_gilded'])\n",
    "        row.append(item['post_gilded_silver'])\n",
    "        row.append(item['post_gilded_gold'])\n",
    "        row.append(item['post_gilded_platinum'])\n",
    "        row.append(item['post_is_original_content'])\n",
    "        row.append(item['post_is_video'])\n",
    "        row.append(item['post_likes'])\n",
    "        row.append(item['post_num_comments'])\n",
    "        row.append(item['post_num_crossposts'])\n",
    "        row.append(item['post_num_reports'])\n",
    "        row.append(item['post_over_18'])\n",
    "        row.append(item['post_pinned'])\n",
    "        row.append(item['subreddit'])\n",
    "        row.append(item['subreddit_subscribers'])\n",
    "        row.append(item['post_ups'])\n",
    "        row.append(item['posted_len_minutes'])\n",
    "\n",
    "        for x in header[21:]:\n",
    "            temp = x.split('*')\n",
    "            contains_ngram = False\n",
    "\n",
    "            if len(temp) == 1:\n",
    "                if temp[0] in item['metadata']:\n",
    "                    contains_ngram = True\n",
    "            if len(temp) == 2:\n",
    "                for y in item['word_bigrams']:\n",
    "                    if y[0] == temp[0] and y[1] == temp[1]:\n",
    "                        contains_ngram = True\n",
    "                        break\n",
    "            if len(temp) == 3:\n",
    "                for y in item['word_trigrams']:\n",
    "                    if y[0] == temp[0] and y[1] == temp[1] and y[2] == temp[2]:\n",
    "                        contains_ngram = True\n",
    "                        break\n",
    "            if len(temp) == 4:\n",
    "                for y in item['word_quadgrams']:\n",
    "                    if y[0] == temp[0] and y[1] == temp[1] and y[2] == temp[2] and y[3] == temp[3]:\n",
    "                        contains_ngram = True\n",
    "                        break\n",
    "\n",
    "            row.append(int(contains_ngram))\n",
    "\n",
    "        csv_data.append(row)\n",
    "    \n",
    "    # Write CSV data to a file\n",
    "    with open('./data/' + file + '.csv', 'w', newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(csv_data)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
