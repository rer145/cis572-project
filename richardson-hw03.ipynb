{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "import re\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import calendar\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_emoji(s):\n",
    "    count = 0\n",
    "    for emoji in UNICODE_EMOJI:\n",
    "        count += s.count(emoji)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:49: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:48: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "with open('../data/richardson-hw01-tweets.json') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "data = []\n",
    "    \n",
    "w_bigrams = []\n",
    "w_trigrams = []\n",
    "w_quadgrams = []\n",
    "\n",
    "c_trigrams = []\n",
    "c_quadgrams = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_txt = tweet['text']\n",
    "    num_emoji = count_emoji(tweet_txt)\n",
    "    \n",
    "    # remove urls\n",
    "    tweet_txt = re.sub('https?:\\/{2}[\\d\\w]+\\.([\\d\\w]+)*(\\/[^\\s]*)*', '', tweet_txt)\n",
    "    \n",
    "    # remove punctuation - including emoji\n",
    "    tweet_txt = re.sub(r'[^\\w\\s]', '', tweet_txt)\n",
    "    \n",
    "    # tokenize into words\n",
    "    tweet_tokens = word_tokenize(tweet_txt)\n",
    "    \n",
    "    # remove stop words\n",
    "    tkn_minus_stop_words = set(tweet_tokens).difference(set(nltk.corpus.stopwords.words('english')))\n",
    "    \n",
    "    # lemmatize existing words\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(t) for t in tkn_minus_stop_words]\n",
    "    \n",
    "    # calculate and aggregate word n-grams\n",
    "    wbg = list(ngrams(tokens, 2))\n",
    "    wtg = list(ngrams(tokens, 3))\n",
    "    wqg = list(ngrams(tokens, 4))\n",
    "    \n",
    "    w_bigrams = w_bigrams + wbg\n",
    "    w_trigrams = w_trigrams + wtg\n",
    "    w_quadgrams = w_quadgrams + wqg\n",
    "    \n",
    "    # calculate and aggregate character n-grams\n",
    "    # use tokens to eliminate spaces\n",
    "    ctg = []\n",
    "    cqg = []\n",
    "    for token in tokens:\n",
    "        chrs = [c for c in token]\n",
    "        ctg = ctg + list(ngrams(chrs, 3))\n",
    "        cqg = cqg + list(ngrams(chrs, 4))\n",
    "        \n",
    "    c_trigrams = c_trigrams + ctg\n",
    "    c_quadgrams = c_quadgrams + cqg\n",
    "    \n",
    "    # calculate date information\n",
    "    posted_on = tweet['created_at']\n",
    "    d1 = datetime.strptime(posted_on, '%a %b %d %H:%M:%S %z %Y')\n",
    "    d2 = datetime.now(timezone.utc)\n",
    "    posted_len_minutes = abs((d1-d2).days * 24 * 60)\n",
    "    \n",
    "    # parse domain names of urls\n",
    "    domains = []\n",
    "    for u in tweet['entities']['urls']:\n",
    "        domains = domains + re.findall('^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)', u['expanded_url'])\n",
    "    \n",
    "    data.append({\n",
    "        'id': tweet['id'],\n",
    "        'username': tweet['user']['screen_name'],\n",
    "        'user_followers': tweet['user']['followers_count'],\n",
    "        'user_verified': tweet['user']['verified'],\n",
    "        'w_bigrams': wbg,\n",
    "        'w_trigrams': wtg,\n",
    "        'w_quadgrams': wqg,\n",
    "        'c_trigrams': ctg,\n",
    "        'c_quadgrams': cqg,\n",
    "        'has_hashtag': len(tweet['entities']['hashtags']) > 0,\n",
    "        'num_hashtags': len(tweet['entities']['hashtags']),\n",
    "        'has_emoji': num_emoji > 0,\n",
    "        'num_emoji': num_emoji,\n",
    "        'has_url': len(tweet['entities']['urls']) > 0,\n",
    "        'url_domains': list(set(domains)),\n",
    "        'mentions_user': len(tweet['entities']['user_mentions']) > 0,\n",
    "        'day_of_week': calendar.day_name[d1.weekday()],\n",
    "        'hour_posted': d1.hour,\n",
    "        'posted_len_minutes': posted_len_minutes,\n",
    "        'retweets': tweet['retweet_count'],\n",
    "        'favorites': tweet['favorite_count']\n",
    "    })\n",
    "    \n",
    "# print(json.dumps(data, indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ngrams:\n",
      "Bigrams: 12493 732\n",
      "Trigrams: 11492 686\n",
      "Quadgrams: 10491 632\n",
      "\n",
      "Character ngrams:\n",
      "Trigrams: 48087 1484\n",
      "Quadgrams: 35711 1558\n",
      "\n",
      "Total ngram features: 118274\n",
      "Total unique ngram features: 5092\n"
     ]
    }
   ],
   "source": [
    "w_bigrams_uniq = set(w_bigrams)\n",
    "w_trigrams_uniq = set(w_trigrams)\n",
    "w_quadgrams_uniq = set(w_quadgrams)\n",
    "\n",
    "c_trigrams_uniq = set(c_trigrams)\n",
    "c_quadgrams_uniq = set(c_quadgrams)\n",
    "\n",
    "print('Word ngrams:')\n",
    "print('Bigrams:', len(w_bigrams), len(w_bigrams_uniq))\n",
    "print('Trigrams:', len(w_trigrams), len(w_trigrams_uniq))\n",
    "print('Quadgrams:', len(w_quadgrams), len(w_quadgrams_uniq))\n",
    "print()\n",
    "print('Character ngrams:')\n",
    "print('Trigrams:', len(c_trigrams), len(c_trigrams_uniq))\n",
    "print('Quadgrams:', len(c_quadgrams), len(c_quadgrams_uniq))\n",
    "print()\n",
    "print('Total ngram features:', len(w_bigrams) + len(w_trigrams) + len(w_quadgrams) + len(c_trigrams) + len(c_quadgrams))\n",
    "print('Total unique ngram features:', len(w_bigrams_uniq) + len(w_trigrams_uniq) + len(w_quadgrams_uniq) + len(c_trigrams_uniq) + len(c_quadgrams_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 5107\n",
      "Total unique ngram features: 5092\n"
     ]
    }
   ],
   "source": [
    "# Build csv file\n",
    "\n",
    "header = ['id', 'username', 'user_followers', 'user_verified', 'has_hashtag', 'num_hashtags', \n",
    "          'has_emoji', 'num_emoji', 'has_url', 'mentions_user', 'day_of_week', \n",
    "          'hour_posted', 'posted_len_minutes', 'retweets', 'favorites']\n",
    "\n",
    "header = header + [x[0] + '*' + x[1] for x in list(w_bigrams_uniq)]\n",
    "header = header + [x[0] + '*' + x[1] + '*' + x[2] for x in list(w_trigrams_uniq)]\n",
    "header = header + [x[0] + '*' + x[1] + '*' + x[2] + '*' + x[3] for x in list(w_quadgrams_uniq)]\n",
    "header = header + [x[0] + '*' + x[1] + '*' + x[2] for x in list(c_trigrams_uniq)]\n",
    "header = header + [x[0] + '*' + x[1] + '*' + x[2] + '*' + x[3] for x in list(c_quadgrams_uniq)]\n",
    "\n",
    "print('Total features:', len(header))\n",
    "print('Total unique ngram features:', len(header[15:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 1002\n",
      "Total features: 5107\n"
     ]
    }
   ],
   "source": [
    "csv_data = [header]\n",
    "\n",
    "for item in data:\n",
    "    row = []\n",
    "    row.append(item['id'])\n",
    "    row.append(item['username'])\n",
    "    row.append(item['user_followers'])\n",
    "    row.append(item['user_verified'])\n",
    "    row.append(item['has_hashtag'])\n",
    "    row.append(item['num_hashtags'])\n",
    "    row.append(item['has_emoji'])\n",
    "    row.append(item['num_emoji'])\n",
    "    row.append(item['has_url'])\n",
    "    # row.append(item['url_domains'])\n",
    "    row.append(item['mentions_user'])\n",
    "    row.append(item['day_of_week'])\n",
    "    row.append(item['hour_posted'])\n",
    "    row.append(item['posted_len_minutes'])\n",
    "    row.append(item['retweets'])\n",
    "    row.append(item['favorites'])\n",
    "    \n",
    "    for x in header[15:]:\n",
    "        temp = x.split('*')\n",
    "        contains_ngram = False\n",
    "        \n",
    "        if len(temp) == 2:\n",
    "            for y in item['w_bigrams']:\n",
    "                if y[0] == temp[0] and y[1] == temp[1]:\n",
    "                    contains_ngram = True\n",
    "                    break\n",
    "        if len(temp) == 3:\n",
    "            if len(temp[0]) == 1:\n",
    "                for y in item['c_trigrams']:\n",
    "                    if y[0] == temp[0] and y[1] == temp[1] and y[2] == temp[2]:\n",
    "                        contains_ngram = True\n",
    "                        break\n",
    "            else:\n",
    "                for y in item['w_trigrams']:\n",
    "                    if y[0] == temp[0] and y[1] == temp[1] and y[2] == temp[2]:\n",
    "                        contains_ngram = True\n",
    "                        break\n",
    "        if len(temp) == 4:\n",
    "            if len(temp[0]) == 1:\n",
    "                for y in item['c_quadgrams']:\n",
    "                    if y[0] == temp[0] and y[1] == temp[1] and y[2] == temp[2] and y[3] == temp[3]:\n",
    "                        contains_ngram = True\n",
    "                        break\n",
    "            else:\n",
    "                for y in item['w_quadgrams']:\n",
    "                    if y[0] == temp[0] and y[1] == temp[1] and y[2] == temp[2] and y[3] == temp[3]:\n",
    "                        contains_ngram = True\n",
    "                        break\n",
    "        \n",
    "        row.append(int(contains_ngram))\n",
    "        \n",
    "    csv_data.append(row)\n",
    "    \n",
    "print('Total records:', len(csv_data))\n",
    "print('Total features:', len(csv_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/richardson-hw03.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(csv_data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
